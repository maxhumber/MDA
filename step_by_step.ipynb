{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our aim is to scrape websites we first have to talk about HTML. Because, behind every web page is an HTML document. While we're not going to write any HTML in this course, we do have to know how to read it. \n",
    "\n",
    "If you're coming from a web development background, or if you've written some HTML, this little introduction will be a breeze! And If you have no idea what HTML is or what it looks like, don't sweat! We'll start at the start. \n",
    "\n",
    "Fire up your favourite web browser (I like Firefox), and bring up [Google](www.google.com):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/google_home.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google is a great case study in HTML because it's famously minimal. To see the HTML that renders the Google home page inside the browser, right click anywhere and select `Inspect Element`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/google_inspect.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will bring up the \"Inspector\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/google_html.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Inspector connects each section of HTML code to each section of the displayed page. Hovering over a piece of code in the Inspector will highlight the linked element inside the browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of `<angled>` brackets in HTML. The Google home page is no exception. The page is riddled with `<div>`, `<span>` and `<style>` tags, each helping, in their own way, to structure and render the result that we see inside the browser. Though Google is (relatively) simple in HTML terms, there's a lot of code in the Inspector that deserves unpacking. We won't. Instead, let's take a couple of gigantic steps back to look at, and appreciate, the minimum amount of boilerplate HTML code required to render a (blank) page:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "    <title></title>\n",
    "  </head>\n",
    "  <body>\n",
    "  </body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of things to note:\n",
    "\n",
    "1. The document type is declared at the top\n",
    "2. The entire page is wrapped in an `<html>` tag\n",
    "3. Open tags (`<tag>`) are eventually followed by close tags (`</tag>`)\n",
    "4. The page is divided into two parts (`head` and `body`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every HTML is pretty well segmented into two parts:\n",
    "\n",
    "- head: metadata and scripts and styling\n",
    "- body: actual content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a more complete page (still not very impressive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "  <head>\n",
       "    <meta charset=\"utf-8\">\n",
       "    <title>This is HTML</title>\n",
       "  </head>\n",
       "  <body>\n",
       "    <h1>This is HTML</h1>\n",
       "    <p>It's not the greatest...</p>\n",
       "    <div class='foo'>...but it is <i>functional</i>.</div>\n",
       "    <br />\n",
       "    <div>For good measure, here's some more of it!</div>\n",
       "    <p>And an image:</p>\n",
       "    <img src='https://invisiblebread.com/comics-firstpanel/2015-03-03-scrape.png' height='200' />\n",
       "    <p id='bar'>Isn't HTML great?!</p>\n",
       "  </body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/bad.html', 'r') as f:\n",
    "    html = f.read()\n",
    "    \n",
    "from IPython.display import HTML; HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This above html document is rendered with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "  <head>\n",
      "    <meta charset=\"utf-8\">\n",
      "    <title>This is HTML</title>\n",
      "  </head>\n",
      "  <body>\n",
      "    <h1>This is HTML</h1>\n",
      "    <p>It's not the greatest...</p>\n",
      "    <div class='foo'>...but it is <i>functional</i>.</div>\n",
      "    <br />\n",
      "    <div>For good measure, here's some more of it!</div>\n",
      "    <p>And an image:</p>\n",
      "    <img src='https://invisiblebread.com/comics-firstpanel/2015-03-03-scrape.png' height='200' />\n",
      "    <p id='bar'>Isn't HTML great?!</p>\n",
      "  </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gazpacho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gazpacho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the various different tags. And now imagine we want to extract information from this page. In order to get all of the `<p>` tags, we would need to import Soup from gazpacho:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gazpacho import Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap the html string in a gazpacho Soup object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = Soup(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use the main \"find\" method on the target tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p>It's not the greatest...</p>,\n",
       " <p>And an image:</p>,\n",
       " <p id=\"bar\">Isn't HTML great?!</p>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find, by default, will return a list if there is more than one element that shares that tag, or a soup object if there's just one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To isolate on specific tags, we can target tag attributes (attrs) in a python dictionary. So, if we're interested in scraping this slice of html: \n",
    "\n",
    "`<p id=\"bar\">Isn't HTML great?!</p>` \n",
    "\n",
    "We can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p id=\"bar\">Isn't HTML great?!</p>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('p', attrs={'id': 'bar'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the text inside the HTML, we can run the `.text` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Isn't HTML great?!\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('p', {'id': 'bar'}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to find all the `div`s on the tag we can do the same thing but with `div` as the first argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"foo\">...but it is <i>functional</i>.</div>,\n",
       " <div>For good measure, here's some more of it!</div>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('div')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get just the first `div`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"foo\">...but it is <i>functional</i>.</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('div', mode='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to isolate the `div` tags that have `class=foo`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'...but it is'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('div', {'class': 'foo'}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can literally isolate any tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'functional'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('i').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But sometimes you want to just get rid of tags, this is accomplished with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'...but it is functional.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('div', {'class': 'foo'}).remove_tags()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML is the stuff of websites. In reality we're not going to import documents from our computer! We're going to have to \"get\" HTML from a website.\n",
    "\n",
    "To get, or download, the HTML from a specific page we'll use get from gazpacho:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gazpacho import get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Status Codes\n",
    "\n",
    "Some common status codes... \n",
    "\n",
    "Everyone is familiar with 404 and maybe 503. Importantly, 200 is the best, you always want 200s. But 400s are your fault and 500s are the website's fault:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1xx Informational\n",
    "- 2xx Sucess\n",
    "    - 200 - OK\n",
    "- 3xx Redirection\n",
    "- 4xx Client Error (a.k.a. **your fault**)\n",
    "    - 400 - Bad Request\n",
    "    - 401 - Unauthorized\n",
    "    - 403 - Forbidden\n",
    "    - 404 - Not Found\n",
    "    - 418 - üçµ\n",
    "    - 429 - Too many requests\n",
    "- 5xx Server Error (a.k.a. **their fault**)\n",
    "    - 500 - Internal Server Error\n",
    "    - 501 - Not Implemented\n",
    "    - 502 - Bad Gateway\n",
    "    - 503 - Service Unavailable\n",
    "    - 504 - Gateway Timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get('https://httpstat.us/403')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get('https://httpstat.us/404')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get('https://httpstat.us/418')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structuring a `get` request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we need to manipulate a URL to return something different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'args': {'colour': 'black', 'year': '2020'},\n",
       " 'data': '',\n",
       " 'files': {},\n",
       " 'form': {},\n",
       " 'headers': {'Accept-Encoding': 'identity',\n",
       "  'Host': 'httpbin.org',\n",
       "  'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0',\n",
       "  'X-Amzn-Trace-Id': 'Root=1-5f46cad4-249a11b862035dddbf7b59b6'},\n",
       " 'json': None,\n",
       " 'method': 'GET',\n",
       " 'origin': '50.101.35.196',\n",
       " 'url': 'https://httpbin.org/anything?year=2020&colour=black'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://httpbin.org/anything?year=2020&colour=black'\n",
    "\n",
    "get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to make this more Pythonic, we can use a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'args': {'colour': 'black', 'year': '2020'},\n",
       " 'data': '',\n",
       " 'files': {},\n",
       " 'form': {},\n",
       " 'headers': {'Accept-Encoding': 'identity',\n",
       "  'Host': 'httpbin.org',\n",
       "  'User-Agent': 'gazpacho',\n",
       "  'X-Amzn-Trace-Id': 'Root=1-5f46cadf-3a9dcfdfc120cf29058da6c0'},\n",
       " 'json': None,\n",
       " 'method': 'GET',\n",
       " 'origin': '50.101.35.196',\n",
       " 'url': 'https://httpbin.org/anything?year=2020&colour=black'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://httpbin.org/anything'\n",
    "\n",
    "r = get(\n",
    "    url, \n",
    "    params={'year': 2020, 'colour': 'black'}, \n",
    "    headers={'User-Agent': 'gazpacho'}\n",
    ")\n",
    "\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Scrape World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem in building a web scraping course is that the web is always changing. It could be that by the time this is published all of the examples are out of date.\n",
    "\n",
    "So, I created a Sandbox for us at: www.scrape.world\n",
    "\n",
    "If, for some reason that site is down ($$) grab the repo here: https://github.com/maxhumber/scrape.world and change all the base urls accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "local = False\n",
    "\n",
    "if local: \n",
    "    url = 'localhost:5000'\n",
    "else:\n",
    "    url = \"https://scrape.world\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First example, scraping all the link tags in the `section-speech`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://en.wikipedia.org/wiki/Alphabet_soup_(linguistics)', 'https://en.wikipedia.org/wiki/Alphabet', 'https://en.wikipedia.org/wiki/Abiogenesis', 'https://en.wikipedia.org/wiki/Soup_kitchen', 'https://en.wikipedia.org/wiki/Stone_soup', 'https://en.wikipedia.org/wiki/Souperism', 'https://en.wikipedia.org/wiki/Great_Famine_(Ireland)', 'https://en.wikipedia.org/wiki/Tag_soup', 'https://en.wikipedia.org/wiki/HTML']\n"
     ]
    }
   ],
   "source": [
    "from gazpacho import get, Soup\n",
    "\n",
    "url = \"https://scrape.world/soup\"\n",
    "html = get(url)\n",
    "soup = Soup(html)\n",
    "\n",
    "fos = soup.find(\"div\", {\"class\": \"section-speech\"})\n",
    "\n",
    "links = []\n",
    "for a in fos.find(\"a\"):\n",
    "    try:\n",
    "        link = a.attrs[\"href\"]\n",
    "        links.append(link)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "links = [l for l in links if \"wikipedia.org\" in l]\n",
    "\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape the total spend for each team:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Toronto Pine Needles', 95929643.0),\n",
       " ('Arizona Dingos', 87349818.0),\n",
       " ('Buffalo Knives', 86968691.0),\n",
       " ('Dallas Celebrities', 82349165.0),\n",
       " ('St. Louis Doldrums', 82862927.0),\n",
       " ('Vancouver Whales', 83580706.0),\n",
       " ('Philadelphia Travellers', 83494245.0),\n",
       " ('Boston Kodiaks', 81394166.0),\n",
       " ('Chicago Greyfalcons', 82984294.0),\n",
       " ('Vegas Shining Templars', 81833332.0),\n",
       " ('Florida Jaguars', 82432002.0),\n",
       " ('San Jose Charlatans', 81395750.0),\n",
       " ('Washington Investments', 80589294.0),\n",
       " ('Edmonton Workers', 80901164.0),\n",
       " ('Detroit Carmine Feathers', 82133668.0),\n",
       " ('Pittsburgh Puffins', 80657875.0),\n",
       " ('Carolina Cyclones', 80405665.0),\n",
       " ('Calgary Flares', 78848375.0),\n",
       " ('Nashville Carnivores', 79779643.0),\n",
       " ('Tampa Bay Thunder', 79103331.0),\n",
       " ('Minnesota Savage', 78420255.0),\n",
       " ('New York Officials', 78837300.0),\n",
       " ('Anaheim Mallards', 78173090.0),\n",
       " ('Montreal Quebecers', 79868809.0),\n",
       " ('Winnipeg Airplanes', 77652021.0),\n",
       " ('Los Angeles Monarchs', 76517727.0),\n",
       " ('New York Indwellers', 76554999.0),\n",
       " ('Ottawa Legislators', 76638547.0),\n",
       " ('Columbus Navy Coats', 77160665.0),\n",
       " ('New Jersey Demons', 73766666.0),\n",
       " ('Colorado Landslide', 74809761.0)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gazpacho import get, Soup\n",
    "\n",
    "url = \"https://scrape.world/spend\"\n",
    "html = get(url)\n",
    "soup = Soup(html)\n",
    "\n",
    "trs = soup.find(\"tr\", {\"class\": \"tmx\"})\n",
    "\n",
    "\n",
    "def parse_tr(tr):\n",
    "    team = tr.find(\"td\", {\"data-label\": \"TEAM\"}).text\n",
    "    spend = float(\n",
    "        tr.find(\"td\", {\"data-label\": \"TODAYS CAP HIT\"}).text.replace(\",\", \"\")[1:]\n",
    "    )\n",
    "    return team, spend\n",
    "\n",
    "\n",
    "spend = [parse_tr(tr) for tr in trs]\n",
    "\n",
    "spend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "# requires some additional setup: https://stackoverflow.com/a/42231328/3731467"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using credentials to log in using Selenium:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing credentials.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile credentials.py\n",
    "\n",
    "from gazpacho import Soup\n",
    "import pandas as pd\n",
    "from selenium.webdriver import Firefox\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "\n",
    "url = \"https://scrape.world/season\"\n",
    "\n",
    "options = Options()\n",
    "options.headless = True\n",
    "browser = Firefox(executable_path=\"/usr/local/bin/geckodriver\", options=options)\n",
    "browser.get(url)\n",
    "\n",
    "# username\n",
    "\n",
    "username = browser.find_element_by_id(\"username\")\n",
    "username.clear()\n",
    "username.send_keys(\"admin\")\n",
    "\n",
    "# password\n",
    "\n",
    "password = browser.find_element_by_name(\"password\")\n",
    "password.clear()\n",
    "password.send_keys(\"admin\")\n",
    "\n",
    "# submit\n",
    "\n",
    "browser.find_element_by_xpath(\"/html/body/div/div/form/div/input[3]\").click()\n",
    "\n",
    "# refetch page (just incase)\n",
    "\n",
    "browser.get(url)\n",
    "\n",
    "html = browser.page_source\n",
    "soup = Soup(html)\n",
    "\n",
    "tables = pd.read_html(browser.page_source)\n",
    "east = tables[0]\n",
    "west = tables[1]\n",
    "df = pd.concat([east, west], axis=0)\n",
    "df[\"W\"] = df[\"W\"].apply(pd.to_numeric, errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"W\"])\n",
    "df = df[[\"Team\", \"W\"]]\n",
    "df = df.rename(columns={\"Team\": \"team\", \"W\": \"wins\"})\n",
    "df = df.sort_values(\"wins\", ascending=False)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        team  wins\r\n",
      "11    Washington Investments  51.9\r\n",
      "2          Tampa Bay Thunder  50.2\r\n",
      "12        Pittsburgh Puffins  49.1\r\n",
      "1             Boston Kodiaks  48.5\r\n",
      "2         Colorado Landslide  47.3\r\n",
      "1         St. Louis Doldrums  46.6\r\n",
      "13       New York Indwellers  46.5\r\n",
      "15         Carolina Cyclones  45.9\r\n",
      "3         Dallas Celebrities  44.8\r\n",
      "3            Florida Jaguars  44.5\r\n",
      "10          Vancouver Whales  44.1\r\n",
      "17   Philadelphia Travellers  43.1\r\n",
      "14       Columbus Navy Coats  43.0\r\n",
      "5       Toronto Pine Needles  42.5\r\n",
      "11          Edmonton Workers  41.9\r\n",
      "12    Vegas Shining Templars  41.6\r\n",
      "18        New York Officials  40.5\r\n",
      "6         Winnipeg Airplanes  40.4\r\n",
      "4       Nashville Carnivores  40.2\r\n",
      "13            Arizona Dingos  40.1\r\n",
      "15            Calgary Flares  39.9\r\n",
      "8           Minnesota Savage  39.0\r\n",
      "7        Chicago Greyfalcons  38.9\r\n",
      "6         Montreal Quebecers  38.8\r\n",
      "16       San Jose Charlatans  36.0\r\n",
      "7             Buffalo Knives  36.0\r\n",
      "17          Anaheim Mallards  35.1\r\n",
      "19         New Jersey Demons  31.9\r\n",
      "18      Los Angeles Monarchs  31.0\r\n",
      "8         Ottawa Legislators  29.9\r\n",
      "9   Detroit Carmine Feathers  21.7\r\n"
     ]
    }
   ],
   "source": [
    "!python credentials.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 Interactions 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interacting with dropdown elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing interactions1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile interactions1.py\n",
    "\n",
    "import time\n",
    "from gazpacho import Soup\n",
    "import pandas as pd\n",
    "from selenium.webdriver import Firefox\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "url = \"https://scrape.world/results\"\n",
    "\n",
    "options = Options()\n",
    "options.headless = True\n",
    "browser = Firefox(executable_path=\"/usr/local/bin/geckodriver\", options=options)\n",
    "browser.get(url)\n",
    "\n",
    "# username\n",
    "\n",
    "username = browser.find_element_by_id(\"username\")\n",
    "username.clear()\n",
    "username.send_keys(\"admin\")\n",
    "time.sleep(0.5)\n",
    "\n",
    "# password\n",
    "\n",
    "password = browser.find_element_by_name(\"password\")\n",
    "password.clear()\n",
    "password.send_keys(\"admin\")\n",
    "time.sleep(0.5)\n",
    "\n",
    "# submit\n",
    "\n",
    "browser.find_element_by_xpath(\"/html/body/div/div/form/div/input[3]\").click()\n",
    "time.sleep(0.5)\n",
    "\n",
    "# refetch page (just incase)\n",
    "\n",
    "browser.get(url)\n",
    "\n",
    "search = browser.find_element_by_xpath(\"/html/body/div/div/div[2]/div[2]/label/input\")\n",
    "search.clear()\n",
    "search.send_keys(\"toronto\")\n",
    "time.sleep(0.5)\n",
    "\n",
    "drop_down = Select(\n",
    "    browser.find_element_by_xpath(\"/html/body/div/div/div[2]/div[1]/label/select\")\n",
    ")\n",
    "drop_down.select_by_visible_text(\"100\")\n",
    "time.sleep(0.5)\n",
    "\n",
    "html = browser.page_source\n",
    "soup = Soup(html)\n",
    "df = pd.read_html(str(soup.find(\"table\")))[0]\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    day                      away  ...  goals_home extra_time_loss\r\n",
      "0     1        Ottawa Legislators  ...           5               0\r\n",
      "1     3      Toronto Pine Needles  ...           1               0\r\n",
      "2     4        Montreal Quebecers  ...           5               1\r\n",
      "3     6        St. Louis Doldrums  ...           2               0\r\n",
      "4     9         Tampa Bay Thunder  ...           3               0\r\n",
      "5    11      Toronto Pine Needles  ...           2               0\r\n",
      "6    14          Minnesota Savage  ...           4               0\r\n",
      "7    15      Toronto Pine Needles  ...           4               0\r\n",
      "8    18            Boston Kodiaks  ...           4               1\r\n",
      "9    20       Columbus Navy Coats  ...           3               1\r\n",
      "10   21      Toronto Pine Needles  ...           4               0\r\n",
      "11   24       San Jose Charlatans  ...           4               0\r\n",
      "12   25      Toronto Pine Needles  ...           5               0\r\n",
      "13   28    Washington Investments  ...           3               1\r\n",
      "14   32      Toronto Pine Needles  ...           3               1\r\n",
      "15   35      Los Angeles Monarchs  ...           3               0\r\n",
      "16   37    Vegas Shining Templars  ...           2               1\r\n",
      "17   39   Philadelphia Travellers  ...           2               1\r\n",
      "18   40      Toronto Pine Needles  ...           5               0\r\n",
      "19   43      Toronto Pine Needles  ...           5               0\r\n",
      "20   45            Boston Kodiaks  ...           2               0\r\n",
      "21   46      Toronto Pine Needles  ...           6               0\r\n",
      "22   49      Toronto Pine Needles  ...           4               0\r\n",
      "23   51      Toronto Pine Needles  ...           1               0\r\n",
      "24   53      Toronto Pine Needles  ...           3               0\r\n",
      "25   57      Toronto Pine Needles  ...           0               0\r\n",
      "26   59      Toronto Pine Needles  ...           6               0\r\n",
      "27   60            Buffalo Knives  ...           2               1\r\n",
      "28   63      Toronto Pine Needles  ...           6               0\r\n",
      "29   64        Colorado Landslide  ...           1               0\r\n",
      "30   67      Toronto Pine Needles  ...           2               0\r\n",
      "31   70      Toronto Pine Needles  ...           1               0\r\n",
      "32   72      Toronto Pine Needles  ...           4               0\r\n",
      "33   74      Toronto Pine Needles  ...           1               0\r\n",
      "34   77            Buffalo Knives  ...           5               0\r\n",
      "35   80      Toronto Pine Needles  ...           3               0\r\n",
      "36   81  Detroit Carmine Feathers  ...           4               0\r\n",
      "37   83         Carolina Cyclones  ...           8               0\r\n",
      "38   87      Toronto Pine Needles  ...           4               1\r\n",
      "39   88        New York Officials  ...           4               1\r\n",
      "40   91      Toronto Pine Needles  ...           1               0\r\n",
      "41   93      Toronto Pine Needles  ...           3               0\r\n",
      "42   95       New York Indwellers  ...           3               0\r\n",
      "43   97          Edmonton Workers  ...           4               0\r\n",
      "44   99        Winnipeg Airplanes  ...           3               1\r\n",
      "45  103      Toronto Pine Needles  ...           8               0\r\n",
      "46  105         New Jersey Demons  ...           7               0\r\n",
      "47  107            Calgary Flares  ...           1               1\r\n",
      "48  109       Chicago Greyfalcons  ...           2               0\r\n",
      "49  118      Toronto Pine Needles  ...           2               0\r\n",
      "50  120      Toronto Pine Needles  ...           3               0\r\n",
      "51  123        Ottawa Legislators  ...           2               1\r\n",
      "52  125           Florida Jaguars  ...           3               0\r\n",
      "53  127      Toronto Pine Needles  ...           5               0\r\n",
      "54  129          Anaheim Mallards  ...           5               1\r\n",
      "55  130      Toronto Pine Needles  ...           2               1\r\n",
      "56  133            Arizona Dingos  ...           3               1\r\n",
      "57  135        Dallas Celebrities  ...           2               0\r\n",
      "\r\n",
      "[58 rows x 6 columns]\r\n"
     ]
    }
   ],
   "source": [
    "!python interactions1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 Interactions 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrolling on the page to load more data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing interactions2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile interactions2.py\n",
    "\n",
    "from gazpacho import Soup\n",
    "import pandas as pd\n",
    "from selenium.webdriver import Firefox\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "base = \"https://scrape.world/\"\n",
    "endpoint = \"population\"\n",
    "url = base + endpoint\n",
    "\n",
    "options = Options()\n",
    "options.headless = True\n",
    "browser = Firefox(executable_path=\"/usr/local/bin/geckodriver\", options=options)\n",
    "browser.get(url)\n",
    "\n",
    "poplist = browser.find_element_by_id('infinite-list')\n",
    "\n",
    "days = 365\n",
    "n = 0\n",
    "while n < 365:\n",
    "    browser.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', poplist)\n",
    "    html = browser.page_source\n",
    "    soup = Soup(html)\n",
    "    n = len(soup.find('ul', {'id': 'infinite-list'}).find('li'))\n",
    "\n",
    "lis = soup.find('ul', {'id': 'infinite-list'}).find('li')\n",
    "\n",
    "def parse_li(li):\n",
    "    day, population = li.text.split(' Population ')\n",
    "    population = int(population)\n",
    "    day = int(day.split('Day ')[-1])\n",
    "    return {'day': day, 'population': population}\n",
    "\n",
    "population = [parse_li(li) for li in lis][:days]\n",
    "print(poplation[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python interactions2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 Downloading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading multimedia elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from shutil import rmtree as delete\n",
    "from urllib.request import urlretrieve as download\n",
    "from gazpacho import get, Soup\n",
    "\n",
    "dir = \"media\"\n",
    "Path(dir).mkdir(exist_ok=True)\n",
    "\n",
    "base = \"https://scrape.world\"\n",
    "url = base + \"/books\"\n",
    "html = get(url)\n",
    "soup = Soup(html)\n",
    "\n",
    "# download images\n",
    "\n",
    "imgs = soup.find(\"img\")\n",
    "srcs = [i.attrs[\"src\"] for i in imgs]\n",
    "\n",
    "for src in srcs:\n",
    "    name = src.split(\"/\")[-1]\n",
    "    download(base + src, f\"{dir}/{name}\")\n",
    "\n",
    "# download audio\n",
    "\n",
    "audio = soup.find(\"audio\").find(\"source\").attrs[\"src\"]\n",
    "name = audio.split(\"/\")[-1]\n",
    "download(base + audio, f\"{dir}/{name}\")\n",
    "\n",
    "# download video\n",
    "\n",
    "video = soup.find(\"video\").find(\"source\").attrs[\"src\"]\n",
    "name = video.split(\"/\")[-1]\n",
    "download(base + video, f\"{dir}/{name}\")\n",
    "\n",
    "# clean up\n",
    "\n",
    "delete(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 Scheduling (Local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape book prices to start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing books.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile books.py\n",
    "\n",
    "from gazpacho import get, Soup\n",
    "import pandas as pd\n",
    "\n",
    "def parse(book):\n",
    "    name = book.find(\"h4\").text\n",
    "    price = float(book.find(\"p\").text[1:].split(\" \")[0])\n",
    "    return name, price\n",
    "\n",
    "def fetch_books():\n",
    "    url = \"https://scrape.world/books\"\n",
    "    html = get(url)\n",
    "    soup = Soup(html)\n",
    "    books = soup.find(\"div\", {\"class\": \"book-\"})\n",
    "    return [parse(book) for book in books]\n",
    "\n",
    "data = fetch_books()\n",
    "books = pd.DataFrame(data, columns=[\"title\", \"price\"])\n",
    "\n",
    "string = f\"Current Prices:\\n```\\n{books.to_markdown(index=False, tablefmt='grid')}\\n```\"\n",
    "\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scheduling** \n",
    "\n",
    "In order to schedule this script to execute at some cadence we'll use `hickory` (`pip install hickory`:\n",
    "\n",
    "```\n",
    "hickory schedule books.py --every=30seconds\n",
    "```\n",
    "\n",
    "To check the status:\n",
    "\n",
    "```\n",
    "hickory status\n",
    "```\n",
    "\n",
    "And to kill:\n",
    "\n",
    "```\n",
    "hickory kill books.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Moving to Slack**\n",
    "\n",
    "To send results to Slack instead of install printing to a log file we'll use `slackclient` the official Slack API for Python:\n",
    "\n",
    "```python\n",
    "pip install slackclient\n",
    "```\n",
    "\n",
    "### Generating a Slack API token \n",
    "\n",
    "In order to build a Slack Bot, we'll need a Slack API token, which will require us to do the following:\n",
    "\n",
    "1. Create a new Slack App\n",
    "\n",
    "Create a [Slack App](https://api.slack.com/apps) by following the link, and clicking **Create New App**.\n",
    "\n",
    "2. Add permissions\n",
    "\n",
    "In the menu on the left, find **OAuth and Permissions**. Click it, and scroll down to the **Scopes** section. Click **Add an OAuth Scope**.\n",
    "\n",
    "Search for the **chat:write** and **chat:write.public** scopes, and add them. At this point, you can install the app to your workspace.\n",
    "\n",
    "3. Copy the token to a `.env` file\n",
    "\n",
    "On the same page you'll find your access token under the label **Bot User OAuth Access Token**. Copy this token, and save it to a `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting booksbot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile booksbot.py\n",
    "\n",
    "import os\n",
    "import sqlite3\n",
    "\n",
    "from gazpacho import get, Soup\n",
    "from dotenv import find_dotenv, load_dotenv # pip install python-dotenv\n",
    "import pandas as pd\n",
    "from slack import WebClient # pip install slackclient\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "con = sqlite3.connect(\"data/books.db\")\n",
    "cur = con.cursor()\n",
    "\n",
    "slack_token = os.environ[\"SLACK_API_TOKEN\"]\n",
    "client = WebClient(token=slack_token)\n",
    "\n",
    "def parse(book):\n",
    "    name = book.find(\"h4\").text\n",
    "    price = float(book.find(\"p\").text[1:].split(\" \")[0])\n",
    "    return name, price\n",
    "\n",
    "def fetch_books():\n",
    "    url = \"https://scrape.world/books\"\n",
    "    html = get(url)\n",
    "    soup = Soup(html)\n",
    "    books = soup.find(\"div\", {\"class\": \"book-\"})\n",
    "    return [parse(book) for book in books]\n",
    "\n",
    "data = fetch_books()\n",
    "books = pd.DataFrame(data, columns=[\"title\", \"price\"])\n",
    "books['date'] = pd.Timestamp(\"now\")\n",
    "\n",
    "books.to_sql('books', con, if_exists='append', index=False)\n",
    "average = pd.read_sql(\"select title, round(avg(price),2) as average from books group by title\", con)\n",
    "df = pd.merge(books[['title', 'price']], average)\n",
    "\n",
    "string = f\"Current Prices:```\\n{df.to_markdown(index=False, tablefmt='grid')}\\n```\"\n",
    "\n",
    "response = client.chat_postMessage(\n",
    "    channel=\"books\",\n",
    "    text=string\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schedule with `hickory schedule booksbot.py --every=30seconds`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09 Serverless (Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert here: demand + AWS Lambda (Chalice)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
