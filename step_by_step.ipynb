{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our aim is to scrape websites we first have to talk about HTML. Because, behind every web page is an HTML document. While we're not going to write any HTML in this course, we do have to know how to read it! \n",
    "\n",
    "If you're coming from a web development background, or if you've written some HTML, this little introduction will be a breeze! And If you have no idea what HTML is or what it looks like, don't sweat! We'll start at the the beginning... \n",
    "\n",
    "Fire up your favourite web browser (I like Firefox), and bring up [Google](www.google.com):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/google_home.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google is a great case study in HTML because it's famously minimal. To see the underlying HTML that renders the Google home page inside the browser, right click anywhere on the page and select `Inspect Element`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/google_inspect.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will bring up the \"Inspector\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/google_html.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Inspector connects each section of HTML code to each section of the displayed page. Hovering over a piece of code in the Inspector will highlight the linked element inside the browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of `<angled>` brackets in HTML. And the Google home page is no exception. The page is riddled with `<div>`, `<span>` and `<style>` tags, each helping, in their own way, to structure and render the result that we see inside the browser. Though Google is (relatively) simple in HTML terms, there's a lot of code in the Inspector that deserves unpacking. We won't. Instead, let's take a couple of gigantic steps back to look at, and appreciate, the minimum amount of boilerplate HTML code required to render a (blank) page:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "    <title></title>\n",
    "  </head>\n",
    "  <body>\n",
    "  </body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of things to note:\n",
    "\n",
    "1. The document type is declared at the top\n",
    "2. The entire page is wrapped in an `<html>` tag\n",
    "3. Open tags (`<tag>`) are eventually followed by close tags (`</tag>`)\n",
    "4. The page is divided into two parts (`head` and `body`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every HTML is pretty well segmented into two parts:\n",
    "\n",
    "- head: metadata and scripts and styling\n",
    "- body: actual content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a more complete page (still not very impressive):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "  <head>\n",
       "    <meta charset=\"utf-8\">\n",
       "    <title>This is HTML</title>\n",
       "  </head>\n",
       "  <body>\n",
       "    <h1>This is HTML</h1>\n",
       "    <p>It's not the greatest...</p>\n",
       "    <div class='foo'>...but it is <i>functional</i>.</div>\n",
       "    <br />\n",
       "    <div>For good measure, here's some more of it!</div>\n",
       "    <p>And an image:</p>\n",
       "    <img src='https://invisiblebread.com/comics-firstpanel/2015-03-03-scrape.png' height='200' />\n",
       "    <p id='bar'>Isn't HTML great?!</p>\n",
       "  </body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/bad.html', 'r') as f:\n",
    "    html = f.read()\n",
    "    \n",
    "from IPython.display import HTML; HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the raw html text we can see the \"page\" rendered with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "  <head>\n",
      "    <meta charset=\"utf-8\">\n",
      "    <title>This is HTML</title>\n",
      "  </head>\n",
      "  <body>\n",
      "    <h1>This is HTML</h1>\n",
      "    <p>It's not the greatest...</p>\n",
      "    <div class='foo'>...but it is <i>functional</i>.</div>\n",
      "    <br />\n",
      "    <div>For good measure, here's some more of it!</div>\n",
      "    <p>And an image:</p>\n",
      "    <img src='https://invisiblebread.com/comics-firstpanel/2015-03-03-scrape.png' height='200' />\n",
      "    <p id='bar'>Isn't HTML great?!</p>\n",
      "  </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gazpacho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the various different tags in the \"This is HTML\" document. And now imagine that we want to extract information from this page. In order to get all of the `<p>` tags, for instance, we'll use a tool called [gazpacho](https://github.com/maxhumber/gazpacho) that can be installed at the command line with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gazpacho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main part of gazpacho is the `Soup` wrapper which allows us to parse over HTML documents, it's imported accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gazpacho import Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable parsing, first wrap the html string in a gazpacho `Soup` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = Soup(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use the main `find` method on the tag you wish to target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p>It's not the greatest...</p>,\n",
       " <p>And an image:</p>,\n",
       " <p id=\"bar\">Isn't HTML great?!</p>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `find` method, by default, will return a list if there is more than one element that shares that tag, or a soup object if there's just one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To isolate on specific tags, we can target tag attributes (`attrs`) in a Python dictionary. So, if we're interested in scraping this slice of html: \n",
    "\n",
    "`<p id=\"bar\">Isn't HTML great?!</p>` \n",
    "\n",
    "We can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p id=\"bar\">Isn't HTML great?!</p>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('p', attrs={'id': 'bar'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the text inside the HTML, we can ask gazpacho to return the `.text` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Isn't HTML great?!\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('p', {'id': 'bar'}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to find all the `div`s on the page we can do the same thing but with `div` as the first argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"foo\">...but it is <i>functional</i>.</div>,\n",
       " <div>For good measure, here's some more of it!</div>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('div')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get just the first `div` (and ignore the rest):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"foo\">...but it is <i>functional</i>.</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('div', mode='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to isolate the `div` tags that have `class=foo`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'...but it is'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('div', {'class': 'foo'}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can literally isolate any tag!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'functional'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('i').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But sometimes you want to just get rid of tags, so this is accomplished by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'...but it is functional.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('div', {'class': 'foo'}).remove_tags()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML is the stuff of websites. Importing HTML documents from our computer is neither fun nor realistic! So let's \"get\" HTML from an actual website.\n",
    "\n",
    "To get, or download the HTML from a specific page we'll use the `get` function from gazpacho:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gazpacho import get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Status Codes\n",
    "\n",
    "If every is hunkydory `get` will just return the raw HTML. But if something is wrong it will raise an HTTP Status code. \n",
    "\n",
    "While everyone is familiar with 404 and maybe 503, here's a helpful list of some common codes that you might encounter in the wild. Most importantly,  400s are your fault and 500s are the website's fault:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1xx Informational\n",
    "- 2xx Sucess\n",
    "    - 200 - OK\n",
    "- 3xx Redirection\n",
    "- 4xx Client Error (a.k.a. **your fault**)\n",
    "    - 400 - Bad Request\n",
    "    - 401 - Unauthorized\n",
    "    - 403 - Forbidden\n",
    "    - 404 - Not Found\n",
    "    - 418 - üçµ\n",
    "    - 429 - Too many requests\n",
    "- 5xx Server Error (a.k.a. **their fault**)\n",
    "    - 500 - Internal Server Error\n",
    "    - 501 - Not Implemented\n",
    "    - 502 - Bad Gateway\n",
    "    - 503 - Service Unavailable\n",
    "    - 504 - Gateway Timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run to see how gazpacho handles HTTP status codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get('https://httpstat.us/403')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get('https://httpstat.us/404')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get('https://httpstat.us/418')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structuring a `get` request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we'll just need to point `get` at a URL. But sometimes, we'll need to manipulate the URL string to return specific information from a page. Here's a query string that perhaps searches for all cars with a year make of 2020 and a colour that equals black:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'args': {'colour': 'black', 'year': '2020'},\n",
       " 'data': '',\n",
       " 'files': {},\n",
       " 'form': {},\n",
       " 'headers': {'Accept-Encoding': 'identity',\n",
       "  'Host': 'httpbin.org',\n",
       "  'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:78.0) Gecko/20100101 Firefox/78.0',\n",
       "  'X-Amzn-Trace-Id': 'Root=1-5f47b813-16dff67d45adcf10433f4cd3'},\n",
       " 'json': None,\n",
       " 'method': 'GET',\n",
       " 'origin': '50.101.35.196',\n",
       " 'url': 'https://httpbin.org/anything?year=2020&colour=black'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://httpbin.org/anything?year=2020&colour=black'\n",
    "\n",
    "get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If instead we wanted red cars made in 2016 we could edit the string, or we could do something a little more Pythonic and use a params dictionary instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'args': {'colour': 'red', 'year': '2016'},\n",
       " 'data': '',\n",
       " 'files': {},\n",
       " 'form': {},\n",
       " 'headers': {'Accept-Encoding': 'identity',\n",
       "  'Host': 'httpbin.org',\n",
       "  'User-Agent': 'gazpacho',\n",
       "  'X-Amzn-Trace-Id': 'Root=1-5f47b863-36773bcc54045184914673e4'},\n",
       " 'json': None,\n",
       " 'method': 'GET',\n",
       " 'origin': '50.101.35.196',\n",
       " 'url': 'https://httpbin.org/anything?year=2016&colour=red'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://httpbin.org/anything'\n",
    "\n",
    "r = get(\n",
    "    url, \n",
    "    params={'year': 2016, 'colour': 'red'}, \n",
    "    headers={'User-Agent': 'gazpacho'}\n",
    ")\n",
    "\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Scrape World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get` requests that we've been looking at are still somewhat artificial... I bet you just want to start scraping already! Me too! But there's a problem...\n",
    "\n",
    "Building a web scraping course is hard. Because by the time this is published it could be that all of the examples are out of date. And it wouldn't be my fault. The web is always changing! \n",
    "\n",
    "So, to solve this problem, I've created a Web Scraping Sandbox that replicates some familiar pages (that won't change) available at: www.scrape.world\n",
    "\n",
    "If, for some reason www.scrape.world is down ($$$) you can grab source code from the repo [here](https://github.com/maxhumber/scrape.world), spin up a local application and change all the base urls accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "local = False\n",
    "\n",
    "if local: \n",
    "    url = 'localhost:5000'\n",
    "else:\n",
    "    url = \"https://scrape.world\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first www.scrape.world example let's scrape all of the link tags in the `section-speech` part of the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wikipedia.org/wiki/Alphabet_soup_(linguistics)',\n",
       " 'https://en.wikipedia.org/wiki/Alphabet',\n",
       " 'https://en.wikipedia.org/wiki/Abiogenesis',\n",
       " 'https://en.wikipedia.org/wiki/Soup_kitchen',\n",
       " 'https://en.wikipedia.org/wiki/Stone_soup',\n",
       " 'https://en.wikipedia.org/wiki/Souperism',\n",
       " 'https://en.wikipedia.org/wiki/Great_Famine_(Ireland)',\n",
       " 'https://en.wikipedia.org/wiki/Tag_soup',\n",
       " 'https://en.wikipedia.org/wiki/HTML']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gazpacho import get, Soup\n",
    "\n",
    "url = \"https://scrape.world/soup\"\n",
    "html = get(url)\n",
    "soup = Soup(html)\n",
    "\n",
    "fos = soup.find(\"div\", {\"class\": \"section-speech\"})\n",
    "\n",
    "links = []\n",
    "for a in fos.find(\"a\"):\n",
    "    try:\n",
    "        link = a.attrs[\"href\"]\n",
    "        links.append(link)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "links = [l for l in links if \"wikipedia.org\" in l]\n",
    "\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we might scrape the total spend for each team on this fictional Salary Cap page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Toronto Pine Needles', 95929643.0),\n",
       " ('Arizona Dingos', 87349818.0),\n",
       " ('Buffalo Knives', 86968691.0),\n",
       " ('Dallas Celebrities', 82349165.0),\n",
       " ('St. Louis Doldrums', 82862927.0),\n",
       " ('Vancouver Whales', 83580706.0),\n",
       " ('Philadelphia Travellers', 83494245.0),\n",
       " ('Boston Kodiaks', 81394166.0),\n",
       " ('Chicago Greyfalcons', 82984294.0),\n",
       " ('Vegas Shining Templars', 81833332.0),\n",
       " ('Florida Jaguars', 82432002.0),\n",
       " ('San Jose Charlatans', 81395750.0),\n",
       " ('Washington Investments', 80589294.0),\n",
       " ('Edmonton Workers', 80901164.0),\n",
       " ('Detroit Carmine Feathers', 82133668.0),\n",
       " ('Pittsburgh Puffins', 80657875.0),\n",
       " ('Carolina Cyclones', 80405665.0),\n",
       " ('Calgary Flares', 78848375.0),\n",
       " ('Nashville Carnivores', 79779643.0),\n",
       " ('Tampa Bay Thunder', 79103331.0),\n",
       " ('Minnesota Savage', 78420255.0),\n",
       " ('New York Officials', 78837300.0),\n",
       " ('Anaheim Mallards', 78173090.0),\n",
       " ('Montreal Quebecers', 79868809.0),\n",
       " ('Winnipeg Airplanes', 77652021.0),\n",
       " ('Los Angeles Monarchs', 76517727.0),\n",
       " ('New York Indwellers', 76554999.0),\n",
       " ('Ottawa Legislators', 76638547.0),\n",
       " ('Columbus Navy Coats', 77160665.0),\n",
       " ('New Jersey Demons', 73766666.0),\n",
       " ('Colorado Landslide', 74809761.0)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gazpacho import get, Soup\n",
    "\n",
    "url = \"https://scrape.world/spend\"\n",
    "html = get(url)\n",
    "soup = Soup(html)\n",
    "\n",
    "trs = soup.find(\"tr\", {\"class\": \"tmx\"})\n",
    "\n",
    "\n",
    "def parse_tr(tr):\n",
    "    team = tr.find(\"td\", {\"data-label\": \"TEAM\"}).text\n",
    "    spend = float(\n",
    "        tr.find(\"td\", {\"data-label\": \"TODAYS CAP HIT\"}).text.replace(\",\", \"\")[1:]\n",
    "    )\n",
    "    return team, spend\n",
    "\n",
    "\n",
    "spend = [parse_tr(tr) for tr in trs]\n",
    "\n",
    "spend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 Credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes what you're looking for is locked behind a login page. So long as you have a user account for that website, we can use Selenium to fake out a browser, capture the rendered HTML, and use gazpacho as normal.\n",
    "\n",
    "To install Selenium run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And follow the additional setup instructions [here](https://stackoverflow.com/a/42231328/3731467)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using credentials to log in using Selenium we can grab the data at the /season endpoint by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing credentials.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile credentials.py\n",
    "\n",
    "from gazpacho import Soup\n",
    "import pandas as pd\n",
    "from selenium.webdriver import Firefox\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "\n",
    "url = \"https://scrape.world/season\"\n",
    "\n",
    "options = Options()\n",
    "options.headless = True\n",
    "browser = Firefox(executable_path=\"/usr/local/bin/geckodriver\", options=options)\n",
    "browser.get(url)\n",
    "\n",
    "# username\n",
    "username = browser.find_element_by_id(\"username\")\n",
    "username.clear()\n",
    "username.send_keys(\"admin\")\n",
    "\n",
    "# password\n",
    "password = browser.find_element_by_name(\"password\")\n",
    "password.clear()\n",
    "password.send_keys(\"admin\")\n",
    "\n",
    "# submit\n",
    "browser.find_element_by_xpath(\"/html/body/div/div/form/div/input[3]\").click()\n",
    "\n",
    "# refetch page (just incase)\n",
    "browser.get(url)\n",
    "\n",
    "html = browser.page_source\n",
    "soup = Soup(html)\n",
    "\n",
    "tables = pd.read_html(browser.page_source)\n",
    "east = tables[0]\n",
    "west = tables[1]\n",
    "df = pd.concat([east, west], axis=0)\n",
    "df[\"W\"] = df[\"W\"].apply(pd.to_numeric, errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"W\"])\n",
    "df = df[[\"Team\", \"W\"]]\n",
    "df = df.rename(columns={\"Team\": \"team\", \"W\": \"wins\"})\n",
    "df = df.sort_values(\"wins\", ascending=False)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python credentials.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 Interactions 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes a website allows us to filter the data displayed on the page with dropdowns and search bars. To interact with dropdown and other page elements we can use Selenium as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing interactions1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile interactions1.py\n",
    "\n",
    "import time\n",
    "from gazpacho import Soup\n",
    "import pandas as pd\n",
    "from selenium.webdriver import Firefox\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "url = \"https://scrape.world/results\"\n",
    "\n",
    "options = Options()\n",
    "options.headless = True\n",
    "browser = Firefox(executable_path=\"/usr/local/bin/geckodriver\", options=options)\n",
    "browser.get(url)\n",
    "\n",
    "# username\n",
    "username = browser.find_element_by_id(\"username\")\n",
    "username.clear()\n",
    "username.send_keys(\"admin\")\n",
    "time.sleep(0.5)\n",
    "\n",
    "# password\n",
    "password = browser.find_element_by_name(\"password\")\n",
    "password.clear()\n",
    "password.send_keys(\"admin\")\n",
    "time.sleep(0.5)\n",
    "\n",
    "# submit\n",
    "browser.find_element_by_xpath(\"/html/body/div/div/form/div/input[3]\").click()\n",
    "time.sleep(0.5)\n",
    "\n",
    "# refetch page (just incase)\n",
    "browser.get(url)\n",
    "\n",
    "search = browser.find_element_by_xpath(\"/html/body/div/div/div[2]/div[2]/label/input\")\n",
    "search.clear()\n",
    "search.send_keys(\"toronto\")\n",
    "time.sleep(0.5)\n",
    "\n",
    "drop_down = Select(\n",
    "    browser.find_element_by_xpath(\"/html/body/div/div/div[2]/div[1]/label/select\")\n",
    ")\n",
    "drop_down.select_by_visible_text(\"100\")\n",
    "time.sleep(0.5)\n",
    "\n",
    "html = browser.page_source\n",
    "soup = Soup(html)\n",
    "df = pd.read_html(str(soup.find(\"table\")))[0]\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python interactions1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 Interactions 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Piggybacking on the last example, here's how we might extract data that iteratively loads on scroll:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile interactions2.py\n",
    "\n",
    "from gazpacho import Soup\n",
    "import pandas as pd\n",
    "from selenium.webdriver import Firefox\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "base = \"https://scrape.world/\"\n",
    "endpoint = \"population\"\n",
    "url = base + endpoint\n",
    "\n",
    "options = Options()\n",
    "options.headless = True\n",
    "browser = Firefox(executable_path=\"/usr/local/bin/geckodriver\", options=options)\n",
    "browser.get(url)\n",
    "\n",
    "poplist = browser.find_element_by_id('infinite-list')\n",
    "\n",
    "days = 365\n",
    "n = 0\n",
    "while n < 365:\n",
    "    browser.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', poplist)\n",
    "    html = browser.page_source\n",
    "    soup = Soup(html)\n",
    "    n = len(soup.find('ul', {'id': 'infinite-list'}).find('li'))\n",
    "\n",
    "lis = soup.find('ul', {'id': 'infinite-list'}).find('li')\n",
    "\n",
    "def parse_li(li):\n",
    "    day, population = li.text.split(' Population ')\n",
    "    population = int(population)\n",
    "    day = int(day.split('Day ')[-1])\n",
    "    return {'day': day, 'population': population}\n",
    "\n",
    "population = [parse_li(li) for li in lis][:days]\n",
    "print(poplation[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python interactions2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 Downloading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we don't want HTML, but instead to extract an image, a video, or an audio clip from a web page. Here's how we might do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from shutil import rmtree as delete\n",
    "from urllib.request import urlretrieve as download\n",
    "from gazpacho import get, Soup\n",
    "\n",
    "dir = \"media\"\n",
    "Path(dir).mkdir(exist_ok=True)\n",
    "\n",
    "base = \"https://scrape.world\"\n",
    "url = base + \"/books\"\n",
    "html = get(url)\n",
    "soup = Soup(html)\n",
    "\n",
    "# download images\n",
    "imgs = soup.find(\"img\")\n",
    "srcs = [i.attrs[\"src\"] for i in imgs]\n",
    "\n",
    "for src in srcs:\n",
    "    name = src.split(\"/\")[-1]\n",
    "    download(base + src, f\"{dir}/{name}\")\n",
    "\n",
    "# download audio\n",
    "audio = soup.find(\"audio\").find(\"source\").attrs[\"src\"]\n",
    "name = audio.split(\"/\")[-1]\n",
    "download(base + audio, f\"{dir}/{name}\")\n",
    "\n",
    "# download video\n",
    "video = soup.find(\"video\").find(\"source\").attrs[\"src\"]\n",
    "name = video.split(\"/\")[-1]\n",
    "download(base + video, f\"{dir}/{name}\")\n",
    "\n",
    "# clean up\n",
    "delete(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 Scheduling (Local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything up until this point has been (hopefully interesting, but nonetheless) table stakes. We want to take our scraping skills to the next level by building a modern web scraper that can run on a schedule. \n",
    "\n",
    "Imagine we want to point our scraper at a page to monitor prices and send us notifications for when a sale is happening. Here's how we'd start building:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile books.py\n",
    "\n",
    "from gazpacho import get, Soup\n",
    "import pandas as pd\n",
    "\n",
    "def parse(book):\n",
    "    name = book.find(\"h4\").text\n",
    "    price = float(book.find(\"p\").text[1:].split(\" \")[0])\n",
    "    return name, price\n",
    "\n",
    "def fetch_books():\n",
    "    url = \"https://scrape.world/books\"\n",
    "    html = get(url)\n",
    "    soup = Soup(html)\n",
    "    books = soup.find(\"div\", {\"class\": \"book-\"})\n",
    "    return [parse(book) for book in books]\n",
    "\n",
    "data = fetch_books()\n",
    "books = pd.DataFrame(data, columns=[\"title\", \"price\"])\n",
    "\n",
    "string = f\"Current Prices:\\n```\\n{books.to_markdown(index=False, tablefmt='grid')}\\n```\"\n",
    "\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scheduling** \n",
    "\n",
    "In order to schedule this script to execute at some cadence we'll use [hickory](https://github.com/maxhumber/hickory) (`pip install hickory`):\n",
    "\n",
    "```\n",
    "hickory schedule books.py --every=30seconds\n",
    "```\n",
    "\n",
    "To check the status of a hickory script, run:\n",
    "\n",
    "```\n",
    "hickory status\n",
    "```\n",
    "\n",
    "And to kill a schedule:\n",
    "\n",
    "```\n",
    "hickory kill books.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Slack over print()**\n",
    "\n",
    "To send results to Slack instead of printing to a log file we'll use [`slackclient`](https://github.com/slackapi/python-slackclient) the official Slack API for Python:\n",
    "\n",
    "```python\n",
    "pip install slackclient\n",
    "```\n",
    "\n",
    "In order to build a Slack Bot, we'll need a Slack API token, which will require us to do the following:\n",
    "\n",
    "1. Create a new Slack App\n",
    "\n",
    "Follow this [link](https://api.slack.com/apps) to open up the Apps Portal and click *Create New App*\n",
    "\n",
    "2. Add permissions\n",
    "\n",
    "In the menu on the left, find *OAuth and Permissions*. Click it, and scroll down to the *Scopes* section. Click *Add an OAuth Scope*.\n",
    "\n",
    "Search for the *chat:write* and *chat:write.public* scopes, and add them. At this point, you can install the app to your workspace.\n",
    "\n",
    "3. Copy the token to a `.env` file\n",
    "\n",
    "On the same page you'll find your access token under the label *Bot User OAuth Access Token*. Copy this token, and save it to a `.env` file\n",
    "\n",
    "It should look like this:\n",
    "\n",
    "```\n",
    "SLACK_API_TOKEN=xoxb-000000000-000000000-a0a0a0a0a0a0a0a0a0a0a0a0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a Slack API token we can now adjust the original Python script to send messages to a Slack Channel of our choosing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile booksbot.py\n",
    "\n",
    "import os\n",
    "import sqlite3\n",
    "\n",
    "from gazpacho import get, Soup\n",
    "from dotenv import find_dotenv, load_dotenv # pip install python-dotenv\n",
    "import pandas as pd\n",
    "from slack import WebClient # pip install slackclient\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "con = sqlite3.connect(\"data/books.db\")\n",
    "cur = con.cursor()\n",
    "\n",
    "slack_token = os.environ[\"SLACK_API_TOKEN\"]\n",
    "client = WebClient(token=slack_token)\n",
    "\n",
    "def parse(book):\n",
    "    name = book.find(\"h4\").text\n",
    "    price = float(book.find(\"p\").text[1:].split(\" \")[0])\n",
    "    return name, price\n",
    "\n",
    "def fetch_books():\n",
    "    url = \"https://scrape.world/books\"\n",
    "    html = get(url)\n",
    "    soup = Soup(html)\n",
    "    books = soup.find(\"div\", {\"class\": \"book-\"})\n",
    "    return [parse(book) for book in books]\n",
    "\n",
    "data = fetch_books()\n",
    "books = pd.DataFrame(data, columns=[\"title\", \"price\"])\n",
    "books['date'] = pd.Timestamp(\"now\")\n",
    "\n",
    "books.to_sql('books', con, if_exists='append', index=False)\n",
    "average = pd.read_sql(\"select title, round(avg(price),2) as average from books group by title\", con)\n",
    "df = pd.merge(books[['title', 'price']], average)\n",
    "\n",
    "string = f\"Current Prices:```\\n{df.to_markdown(index=False, tablefmt='grid')}\\n```\"\n",
    "\n",
    "response = client.chat_postMessage(\n",
    "    channel=\"books\",\n",
    "    text=string\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schedule with `hickory schedule booksbot.py --every=30seconds` to monitor prices on a 30 second cadence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09 Serverless (Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to build an app that scrapes something every day and have it be scheduled on AWS Lambda. Here's what we'll schedule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomorrow='2020-08-28 16:00' demand will be ~6420.0 MW\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "def post(url, data):\n",
    "    data = bytes(json.dumps(data).encode(\"utf-8\"))\n",
    "    request = Request(url=url, data=data, method=\"POST\")\n",
    "    request.add_header(\"Content-type\", \"application/json; charset=UTF-8\")\n",
    "    with urlopen(request) as response:\n",
    "        response = json.loads(response.read().decode(\"utf-8\"))\n",
    "    return response\n",
    "\n",
    "url = \"https://scrape.world/demand\"\n",
    "\n",
    "tomorrow = (pd.Timestamp('today') + pd.Timedelta('1 day')).strftime(\"%Y-%m-%d %H:00\")\n",
    "temperature = 21\n",
    "data = {\"date\": tomorrow, \"temperature\": temperature}\n",
    "response = post(url, data)\n",
    "\n",
    "text = f\"{tomorrow=} demand will be ~{response['demand']} MW\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Serverless**\n",
    "\n",
    "To get it on Lambda we'll use [chalice](https://aws.github.io/chalice/index).\n",
    "\n",
    "**Pricing**\n",
    "\n",
    "The AWS Lambda free usage tier includes [1M free requests per month and 400,000 GB-seconds of compute time per month](https://aws.amazon.com/lambda/pricing/).\n",
    "\n",
    "**Install**\n",
    "\n",
    "To install Chalice, create and activate a virtual environment:\n",
    "\n",
    "```\n",
    "python -m venv venv\n",
    "source venv/bin/activate\n",
    "```\n",
    "\n",
    "And install the Python package with `pip`:\n",
    "\n",
    "```\n",
    "pip install chalice\n",
    "```\n",
    "\n",
    "**Credentials**\n",
    "\n",
    "In order to deploy a Chalice app, you'll need to configure some AWS credentials. If you have previously configured your machine to run boto3 (the AWS SDK for Python) or the AWS CLI then you can skip this section. Otherwise, signup for an AWS account and generate a new key [here](https://console.aws.amazon.com/iam/home?#/security_credentials) (click the Access Keys dropdown)\n",
    "\n",
    "If this is your first time configuring credentials for AWS you can follow these steps to quickly get started:\n",
    "\n",
    "```\n",
    "mkdir ~/.aws\n",
    "cat >> ~/.aws/config\n",
    "[default]\n",
    "aws_access_key_id=YOUR_ACCESS_KEY_HERE\n",
    "aws_secret_access_key=YOUR_SECRET_ACCESS_KEY\n",
    "region=YOUR_REGION (such as ca-central-1, us-east-1, us-west-1, etc)\n",
    "```\n",
    "\n",
    "**Project**\n",
    "\n",
    "With our credntials configured the next thing to do is use the `chalice` command to create a new project, I'm going to call this project `energybot`:\n",
    "\n",
    "```\n",
    "chalice new-project energybot\n",
    "```\n",
    "\n",
    "This will create a `energybot` directory.  \n",
    "\n",
    "`cd` into this directory:\n",
    "\n",
    "```\n",
    "cd energybot\n",
    "```\n",
    "\n",
    "You should see several files that have been created for you:\n",
    "\n",
    "```\n",
    "ls -la\n",
    "drwxr-xr-x   3 max  staff   96 27 Aug 15:38 .chalice\n",
    "-rw-r--r--   1 max  staff   37 27 Aug 15:38 .gitignore\n",
    "-rw-r--r--   1 max  staff  734 27 Aug 15:38 app.py\n",
    "-rw-r--r--   1 max  staff    0 27 Aug 15:38 requirements.txt\n",
    "```\n",
    "\n",
    "You can ignore the `.chalice` directory for now, the two main files we'll focus on is `app.py` and `requirements.txt`.\n",
    "\n",
    "Let's take a look at the `app.py` file:\n",
    "\n",
    "```python\n",
    "from chalice import Chalice\n",
    "\n",
    "app = Chalice(app_name='helloworld')\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return {'hello': 'world'}\n",
    "```\n",
    "\n",
    "The `new-project` command created a sample app that defines a single view, `/`, that when called will return the JSON body `{\"hello\": \"world\"}`.\n",
    "\n",
    "**Deploying**\n",
    "\n",
    "Let's deploy this app.  Make sure you're in the `energybot` directory and run `chalice deploy`:\n",
    "\n",
    "```\n",
    "chalice deploy\n",
    "```\n",
    "\n",
    "> Creating deployment package.\n",
    "> Creating IAM role: helloworld-dev\n",
    "> Creating lambda function: helloworld-dev\n",
    "> Creating Rest API\n",
    "> Resources deployed:\n",
    ">\n",
    ">   - Lambda ARN: arn:aws:lambda:us-west-2:12345:function:helloworld-dev\n",
    ">   - Rest API URL: https://abcd.execute-api.us-west-2.amazonaws.com/api/\n",
    "\n",
    "You now have an API up and running using API Gateway and Lambda:\n",
    "\n",
    "```\n",
    "curl https://9my19y6h9l.execute-api.ca-central-1.amazonaws.com/api/\n",
    "{\"hello\": \"world\"}\n",
    "```\n",
    "\n",
    "Try making a change to the returned dictionary from the `index()` function.  You can then redeploy your changes by running `chalice deploy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Customizing**\n",
    "\n",
    "Let's intergrate our `energy.py` file into the app:\n",
    "\n",
    "```python\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "from chalice import Chalice, Rate\n",
    "import pandas as pd\n",
    "from slack import WebClient\n",
    "\n",
    "app = Chalice(app_name='energybot')\n",
    "\n",
    "if sys.platform == 'darwin':\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "    load_dotenv(find_dotenv())\n",
    "\n",
    "slack_token = os.environ[\"SLACK_API_TOKEN\"]\n",
    "client = WebClient(token=slack_token)\n",
    "\n",
    "def post(url, data):\n",
    "    data = bytes(json.dumps(data).encode(\"utf-8\"))\n",
    "    request = Request(url=url, data=data, method=\"POST\")\n",
    "    request.add_header(\"Content-type\", \"application/json; charset=UTF-8\")\n",
    "    with urlopen(request) as response:\n",
    "        response = json.loads(response.read().decode(\"utf-8\"))\n",
    "    return response\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return {\"hello\": \"world\"}\n",
    "\n",
    "@app.schedule(Rate(1, unit=Rate.MINUTES))\n",
    "def tomorrow_demand(event):\n",
    "    url = \"https://scrape.world/demand\"\n",
    "    tomorrow = (pd.Timestamp('today') + pd.Timedelta('1 day')).strftime(\"%Y-%m-%d %H:00\")\n",
    "    temperature = 21\n",
    "    data = {\"date\": tomorrow, \"temperature\": temperature}\n",
    "    response = post(url, data)\n",
    "    text = f\"{tomorrow=} demand will be ~{response['demand']} MW\"\n",
    "    client.chat_postMessage(channel=\"energy\",text=text)\n",
    "```\n",
    "\n",
    "Install the new requirements into the chalice virtual environment app:\n",
    "\n",
    "```\n",
    "pip install pandas slackclient\n",
    "```\n",
    "\n",
    "pip freeze the dependencies:\n",
    "\n",
    "```\n",
    "pip freeze > requirements.txt\n",
    "```\n",
    "\n",
    "Add `energybot/chalice/config.json` to your `.gitignore` file before adding environment variables.\n",
    "\n",
    "Change the config.json to include your slack api token:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"version\": \"2.0\",\n",
    "  \"app_name\": \"energybot\",\n",
    "  \"stages\": {\n",
    "    \"dev\": {\n",
    "      \"environment_variables\": {\n",
    "        \"SLACK_API_TOKEN\": \"xoxb-1121314111312-1331577646004-shOuF65gofpftXrTaB6WcYAZ\"\n",
    "      },\n",
    "      \"api_gateway_stage\": \"api\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Clean up**\n",
    "\n",
    "If you're done experimenting with Chalice and you'd like to cleanup, you can use the `chalice delete` command, and Chalice will delete all the resources it created when running the `chalice deploy` command.\n",
    "\n",
    "```\n",
    "chalice delete\n",
    "```\n",
    "\n",
    "> Deleting Rest API: abcd4kwyl4\n",
    "> Deleting function aws:arn:lambda:region:123456789:helloworld-dev\n",
    "> Deleting IAM Role helloworld-dev\n",
    "\n",
    "\n",
    "\n",
    "**Appendix A: Note on Rate Schedule**\n",
    "\n",
    "[Source](https://aws.github.io/chalice/api.html?highlight=rate#Rate)\n",
    "\n",
    "An instance of this class can be used as the `expression` value in the [`Chalice.schedule()`](https://aws.github.io/chalice/api.html?highlight=rate#Chalice.schedule) method:\n",
    "\n",
    "```\n",
    "@app.schedule(Rate(5, unit=Rate.MINUTES))\n",
    "def handler(event):\n",
    "    pass\n",
    "```\n",
    "\n",
    "Examples:\n",
    "\n",
    "```\n",
    "# Run every minute.\n",
    "Rate(1, unit=Rate.MINUTES)\n",
    "\n",
    "# Run every 2 hours.\n",
    "Rate(2, unit=Rate.HOURS)\n",
    "```\n",
    "\n",
    "`unit`[¬∂](https://aws.github.io/chalice/api.html?highlight=rate#Rate.unit)\n",
    "\n",
    "The unit of the provided `value` attribute.  This can be either `Rate.MINUTES`, `Rate.HOURS`, or `Rate.DAYS`.\n",
    "\n",
    "**Appendix B: Adding Environment Variables**\n",
    "\n",
    "[Source](https://aws.github.io/chalice/topics/configfile.html?highlight=environment#id1)\n",
    "\n",
    "Adding environment variables to `.chalice/config.json`:\n",
    "\n",
    "In the following example, environment variables are specified both as top level keys as well as per stage.  This allows us to provide environment variables that all stages should have as well as stage specific environment variables:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"version\": \"2.0\",\n",
    "  \"app_name\": \"app\",\n",
    "  \"environment_variables\": {\n",
    "    \"SHARED_CONFIG\": \"foo\",\n",
    "    \"OTHER_CONFIG\": \"from-top\"\n",
    "  },\n",
    "  \"stages\": {\n",
    "    \"dev\": {\n",
    "      \"environment_variables\": {\n",
    "        \"TABLE_NAME\": \"dev-table\",\n",
    "        \"OTHER_CONFIG\": \"dev-value\"\n",
    "      }\n",
    "    },\n",
    "    \"prod\": {\n",
    "      \"environment_variables\": {\n",
    "        \"TABLE_NAME\": \"prod-table\",\n",
    "        \"OTHER_CONFIG\": \"prod-value\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
